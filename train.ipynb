{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ssd_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "res_net50_1 (ResNet50)       multiple                  14785408  \n",
      "_________________________________________________________________\n",
      "conv2d_118 (Conv2D)          multiple                  262400    \n",
      "_________________________________________________________________\n",
      "conv2d_119 (Conv2D)          multiple                  1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_120 (Conv2D)          multiple                  65664     \n",
      "_________________________________________________________________\n",
      "conv2d_121 (Conv2D)          multiple                  295168    \n",
      "_________________________________________________________________\n",
      "conv2d_122 (Conv2D)          multiple                  32896     \n",
      "_________________________________________________________________\n",
      "conv2d_123 (Conv2D)          multiple                  295168    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_124 (Conv2D)          multiple                  460900    \n",
      "_________________________________________________________________\n",
      "conv2d_125 (Conv2D)          multiple                  1382550   \n",
      "_________________________________________________________________\n",
      "conv2d_126 (Conv2D)          multiple                  691350    \n",
      "_________________________________________________________________\n",
      "conv2d_127 (Conv2D)          multiple                  345750    \n",
      "_________________________________________________________________\n",
      "conv2d_128 (Conv2D)          multiple                  230500    \n",
      "_________________________________________________________________\n",
      "conv2d_129 (Conv2D)          multiple                  230500    \n",
      "=================================================================\n",
      "Total params: 20,258,926\n",
      "Trainable params: 20,213,998\n",
      "Non-trainable params: 44,928\n",
      "_________________________________________________________________\n",
      "Epoch: 0/50, step: 0/2141.0, 10.35s/step, loss: 126993.77344, cls loss: 253987.21875, reg loss: 0.32051\n",
      "Epoch: 0/50, step: 1/2141.0, 10.40s/step, loss: 90964.67969, cls loss: 181929.06250, reg loss: 0.29367\n",
      "Epoch: 0/50, step: 2/2141.0, 10.01s/step, loss: 74579.31250, cls loss: 149158.42188, reg loss: 0.19578\n",
      "Epoch: 0/50, step: 3/2141.0, 10.01s/step, loss: 64313.32812, cls loss: 128624.89062, reg loss: 1.76439\n",
      "Epoch: 0/50, step: 4/2141.0, 10.11s/step, loss: 56800.55469, cls loss: 113599.39062, reg loss: 1.72169\n",
      "Epoch: 0/50, step: 5/2141.0, 10.35s/step, loss: 50878.12891, cls loss: 101751.42969, reg loss: 4.83141\n",
      "Epoch: 0/50, step: 6/2141.0, 10.74s/step, loss: 45981.02734, cls loss: 91953.56250, reg loss: 8.49673\n",
      "Epoch: 0/50, step: 7/2141.0, 10.67s/step, loss: 41857.09766, cls loss: 83684.62500, reg loss: 29.57034\n",
      "Epoch: 0/50, step: 8/2141.0, 10.91s/step, loss: 38336.37891, cls loss: 76623.95312, reg loss: 48.80170\n",
      "Epoch: 0/50, step: 9/2141.0, 11.11s/step, loss: 35252.59766, cls loss: 70461.26562, reg loss: 43.92153\n",
      "Epoch: 0/50, step: 10/2141.0, 11.31s/step, loss: 32621.91211, cls loss: 65166.69141, reg loss: 77.12473\n",
      "Epoch: 0/50, step: 11/2141.0, 11.28s/step, loss: 30286.38477, cls loss: 60492.90234, reg loss: 79.86788\n",
      "Epoch: 0/50, step: 12/2141.0, 11.22s/step, loss: 28232.93555, cls loss: 56386.49219, reg loss: 79.37733\n",
      "Epoch: 0/50, step: 13/2141.0, 11.20s/step, loss: 26431.58398, cls loss: 52769.54297, reg loss: 93.62203\n",
      "Epoch: 0/50, step: 14/2141.0, 11.30s/step, loss: 24812.15234, cls loss: 49534.70703, reg loss: 89.59034\n",
      "Epoch: 0/50, step: 15/2141.0, 11.30s/step, loss: 23385.72656, cls loss: 46685.33203, reg loss: 86.11649\n",
      "Epoch: 0/50, step: 16/2141.0, 11.24s/step, loss: 22111.41797, cls loss: 44126.90234, reg loss: 95.92670\n",
      "Epoch: 0/50, step: 17/2141.0, 11.17s/step, loss: 20959.27930, cls loss: 41826.32422, reg loss: 92.22946\n",
      "Epoch: 0/50, step: 18/2141.0, 11.07s/step, loss: 19914.05664, cls loss: 39739.54688, reg loss: 88.55874\n",
      "Epoch: 0/50, step: 19/2141.0, 11.06s/step, loss: 18965.45312, cls loss: 37846.76562, reg loss: 84.13438\n",
      "Epoch: 0/50, step: 20/2141.0, 11.05s/step, loss: 18102.50391, cls loss: 36124.86719, reg loss: 80.13324\n",
      "Epoch: 0/50, step: 21/2141.0, 10.99s/step, loss: 17316.16992, cls loss: 34555.08984, reg loss: 77.24519\n",
      "Epoch: 0/50, step: 22/2141.0, 10.97s/step, loss: 16591.24023, cls loss: 33108.58594, reg loss: 73.88894\n",
      "Epoch: 0/50, step: 23/2141.0, 11.04s/step, loss: 15922.77637, cls loss: 31774.69336, reg loss: 70.85622\n",
      "Epoch: 0/50, step: 24/2141.0, 11.03s/step, loss: 15306.64746, cls loss: 30545.26758, reg loss: 68.02551\n",
      "Epoch: 0/50, step: 25/2141.0, 11.05s/step, loss: 14736.32422, cls loss: 29406.52344, reg loss: 66.12449\n",
      "Epoch: 0/50, step: 26/2141.0, 11.03s/step, loss: 14207.05859, cls loss: 28350.08789, reg loss: 64.03011\n",
      "Epoch: 0/50, step: 27/2141.0, 11.00s/step, loss: 13713.78418, cls loss: 27365.31641, reg loss: 62.25324\n",
      "Epoch: 0/50, step: 28/2141.0, 11.01s/step, loss: 13252.87402, cls loss: 26445.57617, reg loss: 60.17333\n",
      "Epoch: 0/50, step: 29/2141.0, 11.00s/step, loss: 12822.46289, cls loss: 25586.74805, reg loss: 58.17785\n",
      "Epoch: 0/50, step: 30/2141.0, 10.97s/step, loss: 12418.31348, cls loss: 24780.14648, reg loss: 56.47952\n",
      "Epoch: 0/50, step: 31/2141.0, 10.95s/step, loss: 12040.56543, cls loss: 24026.25000, reg loss: 54.88100\n",
      "Epoch: 0/50, step: 32/2141.0, 10.93s/step, loss: 11684.16895, cls loss: 23315.04688, reg loss: 53.29046\n",
      "Epoch: 0/50, step: 33/2141.0, 10.92s/step, loss: 11347.92090, cls loss: 22644.10156, reg loss: 51.74146\n",
      "Epoch: 0/50, step: 34/2141.0, 10.90s/step, loss: 11030.24902, cls loss: 22010.23633, reg loss: 50.26313\n",
      "Epoch: 0/50, step: 35/2141.0, 10.87s/step, loss: 10729.90039, cls loss: 21410.92188, reg loss: 48.87827\n",
      "Epoch: 0/50, step: 36/2141.0, 10.86s/step, loss: 10445.44141, cls loss: 20843.28906, reg loss: 47.59310\n",
      "Epoch: 0/50, step: 37/2141.0, 10.84s/step, loss: 10176.34668, cls loss: 20305.81250, reg loss: 46.87753\n",
      "Epoch: 0/50, step: 38/2141.0, 10.84s/step, loss: 9920.19141, cls loss: 19794.53906, reg loss: 45.84246\n",
      "Epoch: 0/50, step: 39/2141.0, 10.84s/step, loss: 9676.81445, cls loss: 19308.92383, reg loss: 44.70358\n",
      "Epoch: 0/50, step: 40/2141.0, 10.81s/step, loss: 9445.40918, cls loss: 18846.62891, reg loss: 44.18676\n",
      "Epoch: 0/50, step: 41/2141.0, 10.83s/step, loss: 9225.72949, cls loss: 18407.16016, reg loss: 44.29840\n",
      "Epoch: 0/50, step: 42/2141.0, 10.81s/step, loss: 9014.88477, cls loss: 17986.46289, reg loss: 43.30378\n",
      "Epoch: 0/50, step: 43/2141.0, 10.80s/step, loss: 8813.90234, cls loss: 17585.41602, reg loss: 42.38371\n",
      "Epoch: 0/50, step: 44/2141.0, 10.81s/step, loss: 8621.54395, cls loss: 17201.63281, reg loss: 41.45048\n",
      "Epoch: 0/50, step: 45/2141.0, 10.80s/step, loss: 8437.43066, cls loss: 16834.30664, reg loss: 40.54938\n",
      "Epoch: 0/50, step: 46/2141.0, 10.80s/step, loss: 8261.06250, cls loss: 16482.34375, reg loss: 39.77550\n",
      "Epoch: 0/50, step: 47/2141.0, 10.78s/step, loss: 8091.93213, cls loss: 16144.90234, reg loss: 38.95719\n",
      "Epoch: 0/50, step: 48/2141.0, 10.81s/step, loss: 7931.13281, cls loss: 15823.85840, reg loss: 38.40263\n",
      "Epoch: 0/50, step: 49/2141.0, 10.79s/step, loss: 7775.28125, cls loss: 15512.91895, reg loss: 37.63889\n",
      "Epoch: 0/50, step: 50/2141.0, 10.78s/step, loss: 7625.51709, cls loss: 15214.12793, reg loss: 36.90164\n",
      "Epoch: 0/50, step: 51/2141.0, 10.76s/step, loss: 7481.29590, cls loss: 14926.39258, reg loss: 36.19297\n",
      "Epoch: 0/50, step: 52/2141.0, 10.73s/step, loss: 7342.46973, cls loss: 14649.29395, reg loss: 35.64205\n",
      "Epoch: 0/50, step: 53/2141.0, 10.77s/step, loss: 7209.10303, cls loss: 14383.09180, reg loss: 35.11098\n",
      "Epoch: 0/50, step: 54/2141.0, 10.77s/step, loss: 7080.46143, cls loss: 14126.44238, reg loss: 34.47782\n",
      "Epoch: 0/50, step: 55/2141.0, 10.77s/step, loss: 6956.58154, cls loss: 13878.95410, reg loss: 34.20645\n",
      "Epoch: 0/50, step: 56/2141.0, 10.77s/step, loss: 6836.68896, cls loss: 13639.71777, reg loss: 33.65823\n",
      "Epoch: 0/50, step: 57/2141.0, 10.78s/step, loss: 6722.66650, cls loss: 13411.68164, reg loss: 33.64867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/50, step: 58/2141.0, 10.77s/step, loss: 6611.22900, cls loss: 13189.37305, reg loss: 33.08218\n",
      "Epoch: 0/50, step: 59/2141.0, 10.80s/step, loss: 6503.34082, cls loss: 12974.14648, reg loss: 32.53175\n",
      "Epoch: 0/50, step: 60/2141.0, 10.79s/step, loss: 6398.75928, cls loss: 12765.51660, reg loss: 32.00027\n",
      "Epoch: 0/50, step: 61/2141.0, 10.79s/step, loss: 6297.31445, cls loss: 12563.13770, reg loss: 31.48919\n",
      "Epoch: 0/50, step: 62/2141.0, 10.80s/step, loss: 6199.10742, cls loss: 12367.22266, reg loss: 30.99109\n",
      "Epoch: 0/50, step: 63/2141.0, 10.84s/step, loss: 6104.17529, cls loss: 12177.74023, reg loss: 30.60897\n",
      "Epoch: 0/50, step: 64/2141.0, 10.82s/step, loss: 6011.98389, cls loss: 11993.76465, reg loss: 30.20181\n",
      "Epoch: 0/50, step: 65/2141.0, 10.82s/step, loss: 5922.40137, cls loss: 11815.05566, reg loss: 29.74584\n",
      "Epoch: 0/50, step: 66/2141.0, 10.81s/step, loss: 5835.44141, cls loss: 11641.57715, reg loss: 29.30289\n",
      "Epoch: 0/50, step: 67/2141.0, 10.82s/step, loss: 5751.22852, cls loss: 11473.53320, reg loss: 28.92150\n",
      "Epoch: 0/50, step: 68/2141.0, 10.82s/step, loss: 5669.44531, cls loss: 11310.32324, reg loss: 28.56513\n",
      "Epoch: 0/50, step: 69/2141.0, 10.81s/step, loss: 5589.82373, cls loss: 11151.48730, reg loss: 28.15810\n",
      "Epoch: 0/50, step: 70/2141.0, 10.81s/step, loss: 5512.54932, cls loss: 10997.32227, reg loss: 27.77479\n",
      "Epoch: 0/50, step: 71/2141.0, 10.82s/step, loss: 5437.53271, cls loss: 10847.61523, reg loss: 27.44861\n",
      "Epoch: 0/50, step: 72/2141.0, 10.86s/step, loss: 5364.39404, cls loss: 10701.71484, reg loss: 27.07302\n",
      "Epoch: 0/50, step: 73/2141.0, 10.85s/step, loss: 5293.19482, cls loss: 10559.66602, reg loss: 26.72251\n",
      "Epoch: 0/50, step: 74/2141.0, 10.85s/step, loss: 5223.99854, cls loss: 10421.62402, reg loss: 26.37311\n",
      "Epoch: 0/50, step: 75/2141.0, 10.86s/step, loss: 5156.57910, cls loss: 10287.12305, reg loss: 26.03418\n",
      "Epoch: 0/50, step: 76/2141.0, 10.87s/step, loss: 5090.71973, cls loss: 10155.68848, reg loss: 25.75132\n",
      "Epoch: 0/50, step: 77/2141.0, 10.88s/step, loss: 5026.66016, cls loss: 10027.89844, reg loss: 25.42227\n",
      "Epoch: 0/50, step: 78/2141.0, 10.93s/step, loss: 4964.18359, cls loss: 9903.26172, reg loss: 25.10523\n",
      "Epoch: 0/50, step: 79/2141.0, 10.95s/step, loss: 4903.21436, cls loss: 9781.63672, reg loss: 24.79195\n",
      "Epoch: 0/50, step: 80/2141.0, 10.95s/step, loss: 4843.73975, cls loss: 9662.99414, reg loss: 24.48635\n",
      "Epoch: 0/50, step: 81/2141.0, 10.99s/step, loss: 4785.75684, cls loss: 9547.31250, reg loss: 24.20169\n",
      "Epoch: 0/50, step: 82/2141.0, 11.00s/step, loss: 4729.17139, cls loss: 9434.43164, reg loss: 23.91100\n",
      "Epoch: 0/50, step: 83/2141.0, 11.02s/step, loss: 4673.99121, cls loss: 9324.27734, reg loss: 23.70543\n",
      "Epoch: 0/50, step: 84/2141.0, 11.01s/step, loss: 4619.93506, cls loss: 9216.44238, reg loss: 23.42792\n",
      "Epoch: 0/50, step: 85/2141.0, 11.03s/step, loss: 4567.25293, cls loss: 9111.33691, reg loss: 23.16807\n",
      "Epoch: 0/50, step: 86/2141.0, 11.04s/step, loss: 4515.73096, cls loss: 9008.55762, reg loss: 22.90351\n",
      "Epoch: 0/50, step: 87/2141.0, 11.03s/step, loss: 4465.42969, cls loss: 8908.17578, reg loss: 22.68261\n",
      "Epoch: 0/50, step: 88/2141.0, 11.03s/step, loss: 4416.30957, cls loss: 8810.01660, reg loss: 22.60181\n",
      "Epoch: 0/50, step: 89/2141.0, 11.01s/step, loss: 4368.16846, cls loss: 8713.98438, reg loss: 22.35271\n",
      "Epoch: 0/50, step: 90/2141.0, 11.03s/step, loss: 4321.10107, cls loss: 8620.08008, reg loss: 22.12197\n",
      "Epoch: 0/50, step: 91/2141.0, 11.02s/step, loss: 4274.97510, cls loss: 8528.06641, reg loss: 21.88312\n",
      "Epoch: 0/50, step: 92/2141.0, 11.01s/step, loss: 4230.15723, cls loss: 8438.57617, reg loss: 21.73787\n",
      "Epoch: 0/50, step: 93/2141.0, 11.00s/step, loss: 4185.99512, cls loss: 8350.48242, reg loss: 21.50764\n",
      "Epoch: 0/50, step: 94/2141.0, 10.99s/step, loss: 4142.81201, cls loss: 8264.34180, reg loss: 21.28201\n",
      "Epoch: 0/50, step: 95/2141.0, 10.98s/step, loss: 4100.56689, cls loss: 8180.04150, reg loss: 21.09194\n",
      "Epoch: 0/50, step: 96/2141.0, 10.99s/step, loss: 4059.18115, cls loss: 8097.47852, reg loss: 20.88378\n",
      "Epoch: 0/50, step: 97/2141.0, 10.99s/step, loss: 4018.66797, cls loss: 8016.66406, reg loss: 20.67245\n",
      "Epoch: 0/50, step: 98/2141.0, 10.97s/step, loss: 3978.92456, cls loss: 7937.38330, reg loss: 20.46628\n",
      "Epoch: 0/50, step: 99/2141.0, 10.96s/step, loss: 3939.87695, cls loss: 7859.48926, reg loss: 20.26444\n",
      "Epoch: 0/50, step: 100/2141.0, 10.95s/step, loss: 3901.72437, cls loss: 7783.38428, reg loss: 20.06447\n",
      "Epoch: 0/50, step: 101/2141.0, 10.94s/step, loss: 3864.35620, cls loss: 7708.82861, reg loss: 19.88421\n",
      "Epoch: 0/50, step: 102/2141.0, 10.94s/step, loss: 3827.69043, cls loss: 7635.66260, reg loss: 19.71854\n",
      "Epoch: 0/50, step: 103/2141.0, 10.93s/step, loss: 3791.63379, cls loss: 7563.73535, reg loss: 19.53190\n",
      "Epoch: 0/50, step: 104/2141.0, 10.92s/step, loss: 3756.22461, cls loss: 7493.10107, reg loss: 19.34775\n",
      "Epoch: 0/50, step: 105/2141.0, 10.91s/step, loss: 3721.50146, cls loss: 7423.83691, reg loss: 19.16551\n",
      "Epoch: 0/50, step: 106/2141.0, 10.91s/step, loss: 3687.43335, cls loss: 7355.86133, reg loss: 19.00456\n",
      "Epoch: 0/50, step: 107/2141.0, 10.90s/step, loss: 3654.02979, cls loss: 7289.22998, reg loss: 18.82922\n",
      "Epoch: 0/50, step: 108/2141.0, 10.90s/step, loss: 3621.65430, cls loss: 7223.90039, reg loss: 19.40731\n",
      "Epoch: 0/50, step: 109/2141.0, 10.91s/step, loss: 3589.34009, cls loss: 7159.44775, reg loss: 19.23167\n",
      "Epoch: 0/50, step: 110/2141.0, 10.91s/step, loss: 3557.61255, cls loss: 7096.15332, reg loss: 19.07156\n",
      "Epoch: 0/50, step: 111/2141.0, 10.93s/step, loss: 3526.54761, cls loss: 7034.17822, reg loss: 18.91642\n",
      "Epoch: 0/50, step: 112/2141.0, 10.93s/step, loss: 3496.00269, cls loss: 6973.25439, reg loss: 18.75024\n",
      "Epoch: 0/50, step: 113/2141.0, 10.94s/step, loss: 3466.26074, cls loss: 6913.83154, reg loss: 18.68908\n",
      "Epoch: 0/50, step: 114/2141.0, 10.94s/step, loss: 3436.73755, cls loss: 6854.94434, reg loss: 18.52980\n",
      "Epoch: 0/50, step: 115/2141.0, 10.93s/step, loss: 3407.78491, cls loss: 6797.19678, reg loss: 18.37285\n",
      "Epoch: 0/50, step: 116/2141.0, 10.92s/step, loss: 3379.19287, cls loss: 6740.16846, reg loss: 18.21708\n",
      "Epoch: 0/50, step: 117/2141.0, 10.91s/step, loss: 3351.10596, cls loss: 6684.14258, reg loss: 18.06938\n",
      "Epoch: 0/50, step: 118/2141.0, 10.94s/step, loss: 3323.56177, cls loss: 6629.20215, reg loss: 17.92078\n",
      "Epoch: 0/50, step: 119/2141.0, 10.93s/step, loss: 3296.43286, cls loss: 6575.08740, reg loss: 17.77795\n",
      "Epoch: 0/50, step: 120/2141.0, 10.93s/step, loss: 3269.76929, cls loss: 6521.89111, reg loss: 17.64803\n",
      "Epoch: 0/50, step: 121/2141.0, 10.93s/step, loss: 3243.54858, cls loss: 6469.59375, reg loss: 17.50407\n",
      "Epoch: 0/50, step: 122/2141.0, 10.93s/step, loss: 3217.70288, cls loss: 6418.04443, reg loss: 17.36250\n",
      "Epoch: 0/50, step: 123/2141.0, 10.93s/step, loss: 3192.23657, cls loss: 6367.25000, reg loss: 17.22416\n",
      "Epoch: 0/50, step: 124/2141.0, 10.93s/step, loss: 3167.22144, cls loss: 6317.35596, reg loss: 17.08804\n",
      "Epoch: 0/50, step: 125/2141.0, 10.93s/step, loss: 3142.65625, cls loss: 6268.34668, reg loss: 16.96677\n",
      "Epoch: 0/50, step: 126/2141.0, 10.92s/step, loss: 3118.42285, cls loss: 6220.01221, reg loss: 16.83429\n",
      "Epoch: 0/50, step: 127/2141.0, 10.91s/step, loss: 3094.60034, cls loss: 6172.49854, reg loss: 16.70354\n",
      "Epoch: 0/50, step: 128/2141.0, 10.91s/step, loss: 3071.08838, cls loss: 6125.60352, reg loss: 16.57428\n",
      "Epoch: 0/50, step: 129/2141.0, 10.90s/step, loss: 3047.90918, cls loss: 6079.37256, reg loss: 16.44702\n",
      "Epoch: 0/50, step: 130/2141.0, 10.90s/step, loss: 3025.12402, cls loss: 6033.92725, reg loss: 16.32170\n",
      "Epoch: 0/50, step: 131/2141.0, 10.90s/step, loss: 3002.65698, cls loss: 5989.11523, reg loss: 16.19953\n",
      "Epoch: 0/50, step: 132/2141.0, 10.90s/step, loss: 2980.58447, cls loss: 5945.08887, reg loss: 16.08039\n",
      "Epoch: 0/50, step: 133/2141.0, 10.89s/step, loss: 2958.86450, cls loss: 5901.74609, reg loss: 15.98320\n",
      "Epoch: 0/50, step: 134/2141.0, 10.88s/step, loss: 2937.42993, cls loss: 5858.99414, reg loss: 15.86621\n",
      "Epoch: 0/50, step: 135/2141.0, 10.87s/step, loss: 2916.29932, cls loss: 5816.84375, reg loss: 15.75528\n",
      "Epoch: 0/50, step: 136/2141.0, 10.87s/step, loss: 2895.47510, cls loss: 5775.29688, reg loss: 15.65345\n",
      "Epoch: 0/50, step: 137/2141.0, 10.86s/step, loss: 2874.91650, cls loss: 5734.29248, reg loss: 15.54043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/50, step: 138/2141.0, 10.87s/step, loss: 2854.79272, cls loss: 5694.00000, reg loss: 15.58529\n",
      "Epoch: 0/50, step: 139/2141.0, 10.86s/step, loss: 2834.83862, cls loss: 5654.20166, reg loss: 15.47510\n",
      "Epoch: 0/50, step: 140/2141.0, 10.85s/step, loss: 2815.16699, cls loss: 5614.96631, reg loss: 15.36697\n",
      "Epoch: 0/50, step: 141/2141.0, 10.85s/step, loss: 2795.76758, cls loss: 5576.27002, reg loss: 15.26507\n",
      "Epoch: 0/50, step: 142/2141.0, 10.84s/step, loss: 2776.57422, cls loss: 5537.98926, reg loss: 15.15924\n",
      "Epoch: 0/50, step: 143/2141.0, 10.85s/step, loss: 2758.20312, cls loss: 5500.43506, reg loss: 15.97110\n",
      "Epoch: 0/50, step: 144/2141.0, 10.86s/step, loss: 2739.57910, cls loss: 5463.28369, reg loss: 15.87429\n",
      "Epoch: 0/50, step: 145/2141.0, 10.85s/step, loss: 2721.80933, cls loss: 5426.78223, reg loss: 16.83603\n",
      "Epoch: 0/50, step: 146/2141.0, 10.86s/step, loss: 2703.67676, cls loss: 5390.61182, reg loss: 16.74121\n",
      "Epoch: 0/50, step: 147/2141.0, 10.86s/step, loss: 2685.77368, cls loss: 5354.90723, reg loss: 16.63966\n",
      "Epoch: 0/50, step: 148/2141.0, 10.87s/step, loss: 2668.59985, cls loss: 5319.94482, reg loss: 17.25440\n",
      "Epoch: 0/50, step: 149/2141.0, 10.88s/step, loss: 2651.20068, cls loss: 5285.25781, reg loss: 17.14278\n",
      "Epoch: 0/50, step: 150/2141.0, 10.88s/step, loss: 2634.02271, cls loss: 5251.01514, reg loss: 17.03009\n",
      "Epoch: 0/50, step: 151/2141.0, 10.87s/step, loss: 2617.04883, cls loss: 5217.17822, reg loss: 16.91871\n",
      "Epoch: 0/50, step: 152/2141.0, 10.87s/step, loss: 2600.33936, cls loss: 5183.86865, reg loss: 16.80981\n",
      "Epoch: 0/50, step: 153/2141.0, 10.88s/step, loss: 2583.80371, cls loss: 5150.90332, reg loss: 16.70382\n",
      "Epoch: 0/50, step: 154/2141.0, 10.88s/step, loss: 2567.55737, cls loss: 5118.51807, reg loss: 16.59663\n",
      "Epoch: 0/50, step: 155/2141.0, 10.90s/step, loss: 2551.47705, cls loss: 5086.45996, reg loss: 16.49377\n",
      "Epoch: 0/50, step: 156/2141.0, 10.89s/step, loss: 2535.57153, cls loss: 5054.75293, reg loss: 16.38992\n",
      "Epoch: 0/50, step: 157/2141.0, 10.89s/step, loss: 2519.88501, cls loss: 5023.48193, reg loss: 16.28756\n",
      "Epoch: 0/50, step: 158/2141.0, 10.88s/step, loss: 2504.40503, cls loss: 4992.61914, reg loss: 16.19039\n",
      "Epoch: 0/50, step: 159/2141.0, 10.90s/step, loss: 2489.11670, cls loss: 4962.12695, reg loss: 16.10620\n",
      "Epoch: 0/50, step: 160/2141.0, 10.91s/step, loss: 2474.01416, cls loss: 4932.01855, reg loss: 16.00944\n",
      "Epoch: 0/50, step: 161/2141.0, 10.91s/step, loss: 2459.06738, cls loss: 4902.22314, reg loss: 15.91158\n",
      "Epoch: 0/50, step: 162/2141.0, 10.91s/step, loss: 2444.33008, cls loss: 4872.83545, reg loss: 15.82464\n",
      "Epoch: 0/50, step: 163/2141.0, 10.91s/step, loss: 2429.73608, cls loss: 4843.74316, reg loss: 15.72886\n",
      "Epoch: 0/50, step: 164/2141.0, 10.91s/step, loss: 2415.29346, cls loss: 4814.94824, reg loss: 15.63878\n",
      "Epoch: 0/50, step: 165/2141.0, 10.90s/step, loss: 2401.04150, cls loss: 4786.53760, reg loss: 15.54502\n",
      "Epoch: 0/50, step: 166/2141.0, 10.91s/step, loss: 2386.96362, cls loss: 4758.47510, reg loss: 15.45203\n",
      "Epoch: 0/50, step: 167/2141.0, 10.91s/step, loss: 2373.05420, cls loss: 4730.74756, reg loss: 15.36116\n",
      "Epoch: 0/50, step: 168/2141.0, 10.91s/step, loss: 2359.32031, cls loss: 4703.36914, reg loss: 15.27132\n",
      "Epoch: 0/50, step: 169/2141.0, 10.90s/step, loss: 2345.73682, cls loss: 4676.29102, reg loss: 15.18277\n",
      "Epoch: 0/50, step: 170/2141.0, 10.92s/step, loss: 2332.29517, cls loss: 4649.49609, reg loss: 15.09446\n",
      "Epoch: 0/50, step: 171/2141.0, 10.92s/step, loss: 2319.03223, cls loss: 4623.04443, reg loss: 15.02048\n",
      "Epoch: 0/50, step: 172/2141.0, 10.91s/step, loss: 2305.85938, cls loss: 4596.78418, reg loss: 14.93430\n",
      "Epoch: 0/50, step: 173/2141.0, 10.92s/step, loss: 2292.89478, cls loss: 4570.94043, reg loss: 14.84899\n",
      "Epoch: 0/50, step: 174/2141.0, 10.93s/step, loss: 2280.07568, cls loss: 4545.38623, reg loss: 14.76474\n",
      "Epoch: 0/50, step: 175/2141.0, 10.94s/step, loss: 2267.35791, cls loss: 4520.03467, reg loss: 14.68099\n",
      "Epoch: 0/50, step: 176/2141.0, 10.94s/step, loss: 2254.80762, cls loss: 4495.01611, reg loss: 14.59867\n",
      "Epoch: 0/50, step: 177/2141.0, 10.94s/step, loss: 2242.42114, cls loss: 4470.32422, reg loss: 14.51835\n",
      "Epoch: 0/50, step: 178/2141.0, 10.95s/step, loss: 2230.15161, cls loss: 4445.86572, reg loss: 14.43778\n",
      "Epoch: 0/50, step: 179/2141.0, 10.95s/step, loss: 2218.06934, cls loss: 4421.76904, reg loss: 14.36991\n",
      "Epoch: 0/50, step: 180/2141.0, 10.95s/step, loss: 2206.10352, cls loss: 4397.91016, reg loss: 14.29720\n",
      "Epoch: 0/50, step: 181/2141.0, 10.95s/step, loss: 2194.24097, cls loss: 4374.24707, reg loss: 14.23485\n",
      "Epoch: 0/50, step: 182/2141.0, 10.95s/step, loss: 2182.51538, cls loss: 4350.87354, reg loss: 14.15739\n",
      "Epoch: 0/50, step: 183/2141.0, 10.97s/step, loss: 2170.90308, cls loss: 4327.72510, reg loss: 14.08075\n",
      "Epoch: 0/50, step: 184/2141.0, 10.97s/step, loss: 2159.41406, cls loss: 4304.82227, reg loss: 14.00551\n",
      "Epoch: 0/50, step: 185/2141.0, 10.97s/step, loss: 2148.02612, cls loss: 4282.12109, reg loss: 13.93126\n",
      "Epoch: 0/50, step: 186/2141.0, 10.97s/step, loss: 2136.78687, cls loss: 4259.71387, reg loss: 13.85931\n",
      "Epoch: 0/50, step: 187/2141.0, 10.97s/step, loss: 2125.64038, cls loss: 4237.49219, reg loss: 13.78851\n",
      "Epoch: 0/50, step: 188/2141.0, 10.98s/step, loss: 2115.43896, cls loss: 4216.09277, reg loss: 14.78457\n",
      "Epoch: 0/50, step: 189/2141.0, 10.98s/step, loss: 2104.51782, cls loss: 4194.32861, reg loss: 14.70679\n",
      "Epoch: 0/50, step: 190/2141.0, 10.99s/step, loss: 2093.73193, cls loss: 4172.83301, reg loss: 14.63029\n",
      "Epoch: 0/50, step: 191/2141.0, 10.99s/step, loss: 2083.05591, cls loss: 4151.55713, reg loss: 14.55427\n",
      "Epoch: 0/50, step: 192/2141.0, 10.99s/step, loss: 2072.46167, cls loss: 4130.44336, reg loss: 14.47956\n",
      "Epoch: 0/50, step: 193/2141.0, 10.99s/step, loss: 2061.98901, cls loss: 4109.57227, reg loss: 14.40505\n",
      "Epoch: 0/50, step: 194/2141.0, 11.00s/step, loss: 2051.68652, cls loss: 4089.03369, reg loss: 14.33839\n",
      "Epoch: 0/50, step: 195/2141.0, 11.01s/step, loss: 2041.41919, cls loss: 4068.57202, reg loss: 14.26523\n",
      "Epoch: 0/50, step: 196/2141.0, 11.02s/step, loss: 2031.29883, cls loss: 4048.40332, reg loss: 14.19332\n",
      "Epoch: 0/50, step: 197/2141.0, 11.02s/step, loss: 2021.25854, cls loss: 4028.39307, reg loss: 14.12305\n",
      "Epoch: 0/50, step: 198/2141.0, 11.02s/step, loss: 2011.29614, cls loss: 4008.53906, reg loss: 14.05249\n",
      "Epoch: 0/50, step: 199/2141.0, 11.02s/step, loss: 2001.43506, cls loss: 3988.87695, reg loss: 13.99228\n",
      "Epoch: 0/50, step: 200/2141.0, 11.01s/step, loss: 1991.69885, cls loss: 3969.47290, reg loss: 13.92393\n",
      "Epoch: 0/50, step: 201/2141.0, 11.02s/step, loss: 1982.01855, cls loss: 3950.18140, reg loss: 13.85505\n",
      "Epoch: 0/50, step: 202/2141.0, 11.03s/step, loss: 1972.52661, cls loss: 3931.16821, reg loss: 13.88453\n",
      "Epoch: 0/50, step: 203/2141.0, 11.03s/step, loss: 1963.04370, cls loss: 3912.26953, reg loss: 13.81728\n",
      "Epoch: 0/50, step: 204/2141.0, 11.04s/step, loss: 1953.68811, cls loss: 3893.62451, reg loss: 13.75159\n",
      "Epoch: 0/50, step: 205/2141.0, 11.04s/step, loss: 1944.41956, cls loss: 3875.14746, reg loss: 13.69155\n",
      "Epoch: 0/50, step: 206/2141.0, 11.04s/step, loss: 1935.22009, cls loss: 3856.81274, reg loss: 13.62702\n",
      "Epoch: 0/50, step: 207/2141.0, 11.05s/step, loss: 1926.09595, cls loss: 3838.63013, reg loss: 13.56167\n",
      "Epoch: 0/50, step: 208/2141.0, 11.06s/step, loss: 1917.07324, cls loss: 3820.64722, reg loss: 13.49924\n",
      "Epoch: 0/50, step: 209/2141.0, 11.05s/step, loss: 1908.36963, cls loss: 3802.89966, reg loss: 13.83939\n",
      "Epoch: 0/50, step: 210/2141.0, 11.06s/step, loss: 1899.57922, cls loss: 3785.36353, reg loss: 13.79483\n",
      "Epoch: 0/50, step: 211/2141.0, 11.05s/step, loss: 1890.82056, cls loss: 3767.91089, reg loss: 13.73019\n",
      "Epoch: 0/50, step: 212/2141.0, 11.05s/step, loss: 1882.11841, cls loss: 3750.57007, reg loss: 13.66643\n",
      "Epoch: 0/50, step: 213/2141.0, 11.06s/step, loss: 1873.51697, cls loss: 3733.43018, reg loss: 13.60331\n",
      "Epoch: 0/50, step: 214/2141.0, 11.07s/step, loss: 1864.97876, cls loss: 3716.41650, reg loss: 13.54062\n",
      "Epoch: 0/50, step: 215/2141.0, 11.07s/step, loss: 1856.54871, cls loss: 3699.61768, reg loss: 13.47936\n",
      "Epoch: 0/50, step: 216/2141.0, 11.08s/step, loss: 1848.20447, cls loss: 3682.97241, reg loss: 13.43618\n",
      "Epoch: 0/50, step: 217/2141.0, 11.08s/step, loss: 1840.21167, cls loss: 3666.62476, reg loss: 13.79836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/50, step: 218/2141.0, 11.08s/step, loss: 1831.98669, cls loss: 3650.23608, reg loss: 13.73704\n",
      "Epoch: 0/50, step: 219/2141.0, 11.08s/step, loss: 1823.83167, cls loss: 3633.98804, reg loss: 13.67485\n",
      "Epoch: 0/50, step: 220/2141.0, 11.09s/step, loss: 1815.76514, cls loss: 3617.91650, reg loss: 13.61333\n",
      "Epoch: 0/50, step: 221/2141.0, 11.09s/step, loss: 1807.72668, cls loss: 3601.90039, reg loss: 13.55248\n",
      "Epoch: 0/50, step: 222/2141.0, 11.09s/step, loss: 1799.78503, cls loss: 3586.07788, reg loss: 13.49189\n",
      "Epoch: 0/50, step: 223/2141.0, 11.10s/step, loss: 1791.92395, cls loss: 3570.41260, reg loss: 13.43487\n",
      "Epoch: 0/50, step: 224/2141.0, 11.10s/step, loss: 1784.11499, cls loss: 3554.85425, reg loss: 13.37543\n",
      "Epoch: 0/50, step: 225/2141.0, 11.11s/step, loss: 1776.37927, cls loss: 3539.44116, reg loss: 13.31722\n",
      "Epoch: 0/50, step: 226/2141.0, 11.12s/step, loss: 1768.70618, cls loss: 3524.15332, reg loss: 13.25869\n",
      "Epoch: 0/50, step: 227/2141.0, 11.12s/step, loss: 1761.11670, cls loss: 3509.03149, reg loss: 13.20128\n",
      "Epoch: 0/50, step: 228/2141.0, 11.12s/step, loss: 1753.57068, cls loss: 3493.99536, reg loss: 13.14559\n",
      "Epoch: 0/50, step: 229/2141.0, 11.13s/step, loss: 1746.09155, cls loss: 3479.09302, reg loss: 13.08992\n",
      "Epoch: 0/50, step: 230/2141.0, 11.13s/step, loss: 1738.71118, cls loss: 3464.38794, reg loss: 13.03407\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from configuration import IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS, EPOCHS, NUM_CLASSES, BATCH_SIZE, save_model_dir, \\\n",
    "    load_weights_before_training, load_weights_from_epoch, save_frequency, test_images_during_training, \\\n",
    "    test_images_dir_list\n",
    "from core.ground_truth import ReadDataset, MakeGT\n",
    "from core.loss import SSDLoss\n",
    "from core.make_dataset import TFDataset\n",
    "from core.ssd import SSD, ssd_prediction\n",
    "from utils.visualize import visualize_training_results\n",
    "\n",
    "\n",
    "def print_model_summary(network):\n",
    "    network.build(input_shape=(None, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))\n",
    "    network.summary()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # GPU settings\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    dataset = TFDataset()\n",
    "    train_data, train_count = dataset.generate_datatset()\n",
    "\n",
    "    ssd = SSD()\n",
    "    print_model_summary(network=ssd)\n",
    "\n",
    "    if load_weights_before_training:\n",
    "        ssd.load_weights(filepath=save_model_dir+\"epoch-{}\".format(load_weights_from_epoch))\n",
    "        print(\"Successfully load weights!\")\n",
    "    else:\n",
    "        load_weights_from_epoch = -1\n",
    "\n",
    "    # loss\n",
    "    loss = SSDLoss()\n",
    "\n",
    "    # optimizer\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3,\n",
    "                                                                 decay_steps=20000,\n",
    "                                                                 decay_rate=0.96)\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    # metrics\n",
    "    loss_metric = tf.metrics.Mean()\n",
    "    cls_loss_metric = tf.metrics.Mean()\n",
    "    reg_loss_metric = tf.metrics.Mean()\n",
    "\n",
    "    def train_step(batch_images, batch_labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = ssd(batch_images, training=True)\n",
    "            output = ssd_prediction(feature_maps=pred, num_classes=NUM_CLASSES)\n",
    "            gt = MakeGT(batch_labels, pred)\n",
    "            gt_boxes = gt.generate_gt_boxes()\n",
    "            loss_value, cls_loss, reg_loss = loss(y_true=gt_boxes, y_pred=output)\n",
    "        gradients = tape.gradient(loss_value, ssd.trainable_variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(gradients, ssd.trainable_variables))\n",
    "        loss_metric.update_state(values=loss_value)\n",
    "        cls_loss_metric.update_state(values=cls_loss)\n",
    "        reg_loss_metric.update_state(values=reg_loss)\n",
    "\n",
    "\n",
    "    for epoch in range(load_weights_from_epoch + 1, EPOCHS):\n",
    "        start_time = time.time()\n",
    "        for step, batch_data in enumerate(train_data):\n",
    "            images, labels = ReadDataset().read(batch_data)\n",
    "            train_step(batch_images=images, batch_labels=labels)\n",
    "            time_per_step = (time.time() - start_time) / (step + 1)\n",
    "            print(\"Epoch: {}/{}, step: {}/{}, {:.2f}s/step, loss: {:.5f}, \"\n",
    "                  \"cls loss: {:.5f}, reg loss: {:.5f}\".format(epoch,\n",
    "                                                              EPOCHS,\n",
    "                                                              step,\n",
    "                                                              tf.math.ceil(train_count / BATCH_SIZE),\n",
    "                                                              time_per_step,\n",
    "                                                              loss_metric.result(),\n",
    "                                                              cls_loss_metric.result(),\n",
    "                                                              reg_loss_metric.result()))\n",
    "        loss_metric.reset_states()\n",
    "        cls_loss_metric.reset_states()\n",
    "        reg_loss_metric.reset_states()\n",
    "\n",
    "        if epoch % save_frequency == 0:\n",
    "            ssd.save_weights(filepath=save_model_dir+\"epoch-{}\".format(epoch), save_format=\"tf\")\n",
    "\n",
    "        if test_images_during_training:\n",
    "            visualize_training_results(pictures=test_images_dir_list, model=ssd, epoch=epoch)\n",
    "\n",
    "    ssd.save_weights(filepath=save_model_dir+\"saved_model\", save_format=\"tf\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:computerVision]",
   "language": "python",
   "name": "conda-env-computerVision-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
